{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flow_decomposer import FlowDecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#############################################\n",
    "# New test example using mixed Lorenz systems #\n",
    "#############################################\n",
    "\n",
    "def simulate_lorenz(initial, sigma=10, rho=28, beta=8/3, dt=0.01, steps=1000):\n",
    "    \"\"\"\n",
    "    Simulate a Lorenz attractor using Euler integration.\n",
    "    Returns an array of shape (steps, 3).\n",
    "    \"\"\"\n",
    "    trajectory = np.empty((steps, 3))\n",
    "    trajectory[0] = initial\n",
    "    for i in range(1, steps):\n",
    "        x, y, z = trajectory[i-1]\n",
    "        dx = sigma * (y - x)\n",
    "        dy = x * (rho - z) - y\n",
    "        dz = x * y - beta * z\n",
    "        trajectory[i] = trajectory[i-1] + dt * np.array([dx, dy, dz])\n",
    "    return trajectory\n",
    "\n",
    "def test_flow_decomposition():\n",
    "    steps = 10000\n",
    "\n",
    "    # Simulate three independent Lorenz attractors.\n",
    "    traj1 = simulate_lorenz(initial=[1.0, 1.0, 1.0], steps=steps)\n",
    "    traj2 = simulate_lorenz(initial=[-1.0, -1.0, 1.0], steps=steps)\n",
    "    traj3 = simulate_lorenz(initial=[0.5, 0.5, 0.0], steps=steps)\n",
    "    \n",
    "    # Stack the three trajectories horizontally.\n",
    "    # This yields a latent data matrix of shape (steps, 9) [3 systems x 3 dimensions each].\n",
    "    X_latent = np.hstack([traj1, traj2, traj3])\n",
    "    \n",
    "    # Create a mixing matrix with rank 2 so that, although we start with 3 Lorenz systems,\n",
    "    # the effective number of independent dynamical systems in the observables is 2.\n",
    "    observed_dim = 6  # number of observed variables\n",
    "    # Construct M as a product of two matrices to force low rank.\n",
    "    A = np.random.randn(observed_dim, 10)      # (6 x 2)\n",
    "    B = np.random.randn(10, X_latent.shape[1])   # (2 x 9)\n",
    "    M = A @ B  # Resulting mixing matrix of shape (6, 9) with rank at most 2.\n",
    "    \n",
    "    # Mix the latent signals to generate the observed data.\n",
    "    X_mixed = X_latent @ M.T  # shape (steps, observed_dim)\n",
    "    \n",
    "    # Initialization parameters for FlowDecomposition.\n",
    "    init_params = {\n",
    "        \"input_dim\": observed_dim,  # now 6 instead of 4\n",
    "        \"proj_dim\": 1,\n",
    "        \"n_components\": 3,          # expecting two independent latent dynamics\n",
    "        \"num_delays\": 3,\n",
    "        \"delay_step\": 5,\n",
    "        \"subtract_corr\": False,\n",
    "        \"device\": \"cpu\",\n",
    "        \"optimizer\": \"Adagrad\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"random_state\": None\n",
    "    }\n",
    "    \n",
    "    # Fit parameters.\n",
    "    fit_params = {\n",
    "        \"sample_len\": 150,\n",
    "        \"library_len\": 700,\n",
    "        \"exclusion_rad\": 10,\n",
    "        \"theta\": 5,\n",
    "        \"tp\": 20,\n",
    "        \"epochs\": 100,\n",
    "        \"num_batches\": 10,\n",
    "        \"beta\": 0,\n",
    "        \"tp_policy\": \"range\",\n",
    "        \"loss_mask_size\": None\n",
    "    }\n",
    "    \n",
    "    fd = FlowDecomposition(**init_params)\n",
    "    fd.fit(X_mixed, **fit_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_flow_decomposition()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.09902476  0.19804951  0.29707427]\n",
      " [ 0.39609902  0.49512378  0.59414854  0.69317329]\n",
      " [ 0.79219805  0.89122281  0.99024756  1.08927232]\n",
      " ...\n",
      " [97.91072768 98.00975244 98.10877719 98.20780195]\n",
      " [98.30682671 98.40585146 98.50487622 98.60390098]\n",
      " [98.70292573 98.80195049 98.90097524 99.        ]]\n",
      "Epoch 1/100, Loss: 1.7310, ccm_loss: 0.9630, h_norm_loss: 0.7680\n",
      "Epoch 2/100, Loss: 1.7386, ccm_loss: 0.9749, h_norm_loss: 0.7637\n",
      "Epoch 3/100, Loss: 1.7245, ccm_loss: 0.9641, h_norm_loss: 0.7604\n",
      "Epoch 4/100, Loss: 1.7135, ccm_loss: 0.9564, h_norm_loss: 0.7571\n",
      "Epoch 5/100, Loss: 1.7222, ccm_loss: 0.9683, h_norm_loss: 0.7539\n",
      "Epoch 6/100, Loss: 1.7088, ccm_loss: 0.9590, h_norm_loss: 0.7498\n",
      "Epoch 7/100, Loss: 1.7037, ccm_loss: 0.9579, h_norm_loss: 0.7458\n",
      "Epoch 8/100, Loss: 1.7050, ccm_loss: 0.9630, h_norm_loss: 0.7420\n",
      "Epoch 9/100, Loss: 1.6840, ccm_loss: 0.9463, h_norm_loss: 0.7377\n",
      "Epoch 10/100, Loss: 1.6878, ccm_loss: 0.9528, h_norm_loss: 0.7350\n",
      "Epoch 11/100, Loss: 1.6824, ccm_loss: 0.9474, h_norm_loss: 0.7350\n",
      "Epoch 12/100, Loss: 1.7140, ccm_loss: 0.9790, h_norm_loss: 0.7350\n",
      "Epoch 13/100, Loss: 1.6758, ccm_loss: 0.9407, h_norm_loss: 0.7351\n",
      "Epoch 14/100, Loss: 1.6938, ccm_loss: 0.9587, h_norm_loss: 0.7351\n",
      "Epoch 15/100, Loss: 1.6983, ccm_loss: 0.9633, h_norm_loss: 0.7351\n",
      "Epoch 16/100, Loss: 1.6919, ccm_loss: 0.9569, h_norm_loss: 0.7351\n",
      "Epoch 17/100, Loss: 1.6987, ccm_loss: 0.9637, h_norm_loss: 0.7351\n",
      "Epoch 18/100, Loss: 1.6871, ccm_loss: 0.9520, h_norm_loss: 0.7351\n",
      "Epoch 19/100, Loss: 1.7020, ccm_loss: 0.9669, h_norm_loss: 0.7351\n",
      "Epoch 20/100, Loss: 1.6889, ccm_loss: 0.9539, h_norm_loss: 0.7351\n",
      "Epoch 21/100, Loss: 1.6697, ccm_loss: 0.9346, h_norm_loss: 0.7351\n",
      "Epoch 22/100, Loss: 1.6783, ccm_loss: 0.9432, h_norm_loss: 0.7351\n",
      "Epoch 23/100, Loss: 1.6957, ccm_loss: 0.9606, h_norm_loss: 0.7351\n",
      "Epoch 24/100, Loss: 1.6956, ccm_loss: 0.9605, h_norm_loss: 0.7351\n",
      "Epoch 25/100, Loss: 1.6852, ccm_loss: 0.9501, h_norm_loss: 0.7351\n",
      "Epoch 26/100, Loss: 1.6575, ccm_loss: 0.9224, h_norm_loss: 0.7351\n",
      "Epoch 27/100, Loss: 1.6959, ccm_loss: 0.9608, h_norm_loss: 0.7351\n",
      "Epoch 28/100, Loss: 1.7062, ccm_loss: 0.9711, h_norm_loss: 0.7351\n",
      "Epoch 29/100, Loss: 1.7072, ccm_loss: 0.9722, h_norm_loss: 0.7351\n",
      "Epoch 30/100, Loss: 1.7007, ccm_loss: 0.9656, h_norm_loss: 0.7351\n",
      "Epoch 31/100, Loss: 1.7011, ccm_loss: 0.9660, h_norm_loss: 0.7351\n",
      "Epoch 32/100, Loss: 1.6991, ccm_loss: 0.9640, h_norm_loss: 0.7351\n",
      "Epoch 33/100, Loss: 1.6740, ccm_loss: 0.9389, h_norm_loss: 0.7351\n",
      "Epoch 34/100, Loss: 1.6844, ccm_loss: 0.9493, h_norm_loss: 0.7351\n",
      "Epoch 35/100, Loss: 1.6946, ccm_loss: 0.9595, h_norm_loss: 0.7351\n",
      "Epoch 36/100, Loss: 1.6769, ccm_loss: 0.9418, h_norm_loss: 0.7351\n",
      "Epoch 37/100, Loss: 1.6667, ccm_loss: 0.9316, h_norm_loss: 0.7351\n",
      "Epoch 38/100, Loss: 1.6961, ccm_loss: 0.9611, h_norm_loss: 0.7351\n",
      "Epoch 39/100, Loss: 1.7016, ccm_loss: 0.9665, h_norm_loss: 0.7351\n",
      "Epoch 40/100, Loss: 1.7048, ccm_loss: 0.9698, h_norm_loss: 0.7351\n",
      "Epoch 41/100, Loss: 1.7025, ccm_loss: 0.9674, h_norm_loss: 0.7351\n",
      "Epoch 42/100, Loss: 1.6769, ccm_loss: 0.9418, h_norm_loss: 0.7351\n",
      "Epoch 43/100, Loss: 1.6910, ccm_loss: 0.9559, h_norm_loss: 0.7351\n",
      "Epoch 44/100, Loss: 1.6785, ccm_loss: 0.9435, h_norm_loss: 0.7351\n",
      "Epoch 45/100, Loss: 1.6900, ccm_loss: 0.9549, h_norm_loss: 0.7351\n",
      "Epoch 46/100, Loss: 1.6888, ccm_loss: 0.9537, h_norm_loss: 0.7351\n",
      "Epoch 47/100, Loss: 1.6675, ccm_loss: 0.9324, h_norm_loss: 0.7351\n",
      "Epoch 48/100, Loss: 1.6759, ccm_loss: 0.9408, h_norm_loss: 0.7351\n",
      "Epoch 49/100, Loss: 1.6915, ccm_loss: 0.9564, h_norm_loss: 0.7351\n",
      "Epoch 50/100, Loss: 1.6855, ccm_loss: 0.9504, h_norm_loss: 0.7351\n",
      "Epoch 51/100, Loss: 1.6948, ccm_loss: 0.9597, h_norm_loss: 0.7351\n",
      "Epoch 52/100, Loss: 1.6897, ccm_loss: 0.9546, h_norm_loss: 0.7351\n",
      "Epoch 53/100, Loss: 1.6681, ccm_loss: 0.9330, h_norm_loss: 0.7351\n",
      "Epoch 54/100, Loss: 1.6855, ccm_loss: 0.9504, h_norm_loss: 0.7351\n",
      "Epoch 55/100, Loss: 1.6944, ccm_loss: 0.9593, h_norm_loss: 0.7351\n",
      "Epoch 56/100, Loss: 1.6853, ccm_loss: 0.9502, h_norm_loss: 0.7351\n",
      "Epoch 57/100, Loss: 1.6817, ccm_loss: 0.9466, h_norm_loss: 0.7351\n",
      "Epoch 58/100, Loss: 1.6710, ccm_loss: 0.9359, h_norm_loss: 0.7351\n",
      "Epoch 59/100, Loss: 1.6875, ccm_loss: 0.9524, h_norm_loss: 0.7351\n",
      "Epoch 60/100, Loss: 1.6891, ccm_loss: 0.9540, h_norm_loss: 0.7351\n",
      "Epoch 61/100, Loss: 1.6735, ccm_loss: 0.9384, h_norm_loss: 0.7351\n",
      "Epoch 62/100, Loss: 1.6866, ccm_loss: 0.9515, h_norm_loss: 0.7351\n",
      "Epoch 63/100, Loss: 1.7020, ccm_loss: 0.9669, h_norm_loss: 0.7351\n",
      "Epoch 64/100, Loss: 1.6925, ccm_loss: 0.9574, h_norm_loss: 0.7351\n",
      "Epoch 65/100, Loss: 1.6926, ccm_loss: 0.9575, h_norm_loss: 0.7351\n",
      "Epoch 66/100, Loss: 1.7053, ccm_loss: 0.9702, h_norm_loss: 0.7351\n",
      "Epoch 67/100, Loss: 1.6755, ccm_loss: 0.9405, h_norm_loss: 0.7351\n",
      "Epoch 68/100, Loss: 1.6877, ccm_loss: 0.9526, h_norm_loss: 0.7351\n",
      "Epoch 69/100, Loss: 1.6856, ccm_loss: 0.9505, h_norm_loss: 0.7351\n",
      "Epoch 70/100, Loss: 1.7017, ccm_loss: 0.9666, h_norm_loss: 0.7351\n",
      "Epoch 71/100, Loss: 1.6914, ccm_loss: 0.9564, h_norm_loss: 0.7351\n",
      "Epoch 72/100, Loss: 1.6784, ccm_loss: 0.9434, h_norm_loss: 0.7351\n",
      "Epoch 73/100, Loss: 1.6945, ccm_loss: 0.9594, h_norm_loss: 0.7351\n",
      "Epoch 74/100, Loss: 1.6913, ccm_loss: 0.9562, h_norm_loss: 0.7351\n",
      "Epoch 75/100, Loss: 1.7046, ccm_loss: 0.9695, h_norm_loss: 0.7351\n",
      "Epoch 76/100, Loss: 1.6849, ccm_loss: 0.9498, h_norm_loss: 0.7351\n",
      "Epoch 77/100, Loss: 1.6777, ccm_loss: 0.9426, h_norm_loss: 0.7351\n",
      "Epoch 78/100, Loss: 1.6915, ccm_loss: 0.9565, h_norm_loss: 0.7351\n",
      "Epoch 79/100, Loss: 1.6955, ccm_loss: 0.9604, h_norm_loss: 0.7351\n",
      "Epoch 80/100, Loss: 1.6674, ccm_loss: 0.9323, h_norm_loss: 0.7351\n",
      "Epoch 81/100, Loss: 1.6859, ccm_loss: 0.9509, h_norm_loss: 0.7351\n",
      "Epoch 82/100, Loss: 1.6602, ccm_loss: 0.9251, h_norm_loss: 0.7351\n",
      "Epoch 83/100, Loss: 1.7038, ccm_loss: 0.9687, h_norm_loss: 0.7351\n",
      "Epoch 84/100, Loss: 1.6567, ccm_loss: 0.9217, h_norm_loss: 0.7351\n",
      "Epoch 85/100, Loss: 1.6907, ccm_loss: 0.9556, h_norm_loss: 0.7351\n",
      "Epoch 86/100, Loss: 1.6994, ccm_loss: 0.9643, h_norm_loss: 0.7351\n",
      "Epoch 87/100, Loss: 1.6977, ccm_loss: 0.9626, h_norm_loss: 0.7351\n",
      "Epoch 88/100, Loss: 1.6826, ccm_loss: 0.9475, h_norm_loss: 0.7351\n",
      "Epoch 89/100, Loss: 1.6956, ccm_loss: 0.9605, h_norm_loss: 0.7351\n",
      "Epoch 90/100, Loss: 1.6831, ccm_loss: 0.9480, h_norm_loss: 0.7351\n",
      "Epoch 91/100, Loss: 1.6883, ccm_loss: 0.9532, h_norm_loss: 0.7351\n",
      "Epoch 92/100, Loss: 1.6763, ccm_loss: 0.9413, h_norm_loss: 0.7351\n",
      "Epoch 93/100, Loss: 1.6971, ccm_loss: 0.9620, h_norm_loss: 0.7351\n",
      "Epoch 94/100, Loss: 1.6946, ccm_loss: 0.9595, h_norm_loss: 0.7351\n",
      "Epoch 95/100, Loss: 1.6876, ccm_loss: 0.9526, h_norm_loss: 0.7351\n",
      "Epoch 96/100, Loss: 1.6875, ccm_loss: 0.9524, h_norm_loss: 0.7351\n",
      "Epoch 97/100, Loss: 1.6929, ccm_loss: 0.9578, h_norm_loss: 0.7351\n",
      "Epoch 98/100, Loss: 1.6989, ccm_loss: 0.9639, h_norm_loss: 0.7351\n",
      "Epoch 99/100, Loss: 1.6924, ccm_loss: 0.9573, h_norm_loss: 0.7351\n",
      "Epoch 100/100, Loss: 1.6994, ccm_loss: 0.9643, h_norm_loss: 0.7351\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create dummy data: a time series with 100 points and 1 feature.\n",
    "X = np.linspace(0, 99, 4000).reshape(1000,4).T.reshape(1000,4)\n",
    "print(X)\n",
    "\n",
    "# Initialization parameters dictionary.\n",
    "init_params = {\n",
    "    \"input_dim\": 4,\n",
    "    \"proj_dim\": 2,\n",
    "    \"n_components\": 1,\n",
    "    \"num_delays\": None,\n",
    "    \"delay_step\": None,\n",
    "    \"subtract_corr\": False,\n",
    "    \"device\": \"cpu\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"random_state\": None\n",
    "}\n",
    "\n",
    "# Fit parameters dictionary.\n",
    "fit_params = {\n",
    "    \"sample_len\": 50,\n",
    "    \"library_len\": 700,\n",
    "    \"exclusion_rad\": 1,\n",
    "    \"theta\": 3,\n",
    "    \"tp\": 20,\n",
    "    \"epochs\": 100,\n",
    "    \"num_batches\": 20,\n",
    "    \"beta\": 1,\n",
    "    \"tp_policy\": \"range\",\n",
    "    \"loss_mask_size\": None\n",
    "}\n",
    "\n",
    "fd = FlowDecomposition(**init_params)\n",
    "fd.fit(X, **fit_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FlowDec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
